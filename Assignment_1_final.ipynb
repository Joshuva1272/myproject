{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2cba978-e7f7-4631-ac58-b0b915178e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Global Setup: imports, RNG, and small helpers =====\n",
    "import numpy as np\n",
    "\n",
    "# Reproducibility\n",
    "RNG = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f7f3051-d37c-43e6-a52c-6fa5c8e9958f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1      20000    2          2         1   24      2      2     -1     -1   \n",
       "1   2     120000    2          2         2   26     -1      2      0      0   \n",
       "2   3      90000    2          2         2   34      0      0      0      0   \n",
       "3   4      50000    2          2         1   37      0      0      0      0   \n",
       "4   5      50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...          0          0          0         0       689         0   \n",
       "1  ...       3272       3455       3261         0      1000      1000   \n",
       "2  ...      14331      14948      15549      1518      1500      1000   \n",
       "3  ...      28314      28959      29547      2000      2019      1200   \n",
       "4  ...      20940      19146      19131      2000     36681     10000   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "0         0         0         0                           1  \n",
       "1      1000         0      2000                           1  \n",
       "2      1000      1000      5000                           0  \n",
       "3      1100      1069      1000                           0  \n",
       "4      9000       689       679                           0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import pandas library first\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r\"C:\\Users\\JOSHUVA\\OneDrive\\Desktop\\Study Materials\\Machine Learning\\Assignment_1\\default+of+credit+card+clients\\default of credit card clients.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dae04ca3-7d78-4185-9998-96e03071cdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: X.shape=(30000, 91), y.shape=(30000,), positives=6636 (22.12%)\n",
      "\n",
      "=== Perceptron ===\n",
      "Accuracy: 0.7553 | Precision: 0.4061 | Recall: 0.2298 | F1: 0.2936\n",
      "Confusion Matrix:\n",
      " [[4227  446]\n",
      " [1022  305]]\n",
      "\n",
      "=== Logistic Regression (GD) ===\n",
      "Accuracy: 0.8172 | Precision: 0.6681 | Recall: 0.3444 | F1: 0.4545\n",
      "Confusion Matrix:\n",
      " [[4446  227]\n",
      " [ 870  457]]\n",
      "\n",
      "=== Simple Neural Network (1 hidden layer) ===\n",
      "Accuracy: 0.8100 | Precision: 0.6398 | Recall: 0.3225 | F1: 0.4289\n",
      "Confusion Matrix:\n",
      " [[4432  241]\n",
      " [ 899  428]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ====== USER CONFIG ======\n",
    "CSV_PATH = r\"C:\\Users\\JOSHUVA\\OneDrive\\Desktop\\Study Materials\\Machine Learning\\Assignment_1\\default+of+credit+card+clients\\default of credit card clients.csv\"\n",
    "TEST_SIZE = 0.2\n",
    "SEED = 42\n",
    "# =========================\n",
    "\n",
    "# Known names in the dataset\n",
    "KNOWN_TARGET_NAMES = {\n",
    "    'default payment next month',\n",
    "    'default_payment_next_month',\n",
    "    'default',\n",
    "}\n",
    "KNOWN_DROP_COLS = {'id'}\n",
    "CATEGORICAL_NAMES = {\n",
    "    'SEX', 'EDUCATION', 'MARRIAGE',\n",
    "    'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6'\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers: string processing\n",
    "# -----------------------------\n",
    "def _safe_name(s):\n",
    "    return s.strip().lower().replace(' ', '_')\n",
    "\n",
    "def _upper(s):\n",
    "    return s.strip().upper()\n",
    "\n",
    "def _try_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# -----------------------------\n",
    "# Loading CSV (NumPy + builtins only)\n",
    "# -----------------------------\n",
    "def read_csv_numpy_only(path, assume_header=True):\n",
    "    \"\"\"\n",
    "    Reads a CSV using only builtins (open/read/split) + NumPy.\n",
    "    Returns (header_list, data_2d_list_of_floats).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8-sig') as f:\n",
    "            lines = f.read().splitlines()\n",
    "    except Exception as e:\n",
    "        print(\"Failed to read CSV. Check CSV_PATH.\")\n",
    "        print(e)\n",
    "        return None, None\n",
    "\n",
    "    if not lines:\n",
    "        print(\"CSV is empty.\")\n",
    "        return None, None\n",
    "\n",
    "    header = None\n",
    "    start = 0\n",
    "    if assume_header:\n",
    "        header = [h.strip() for h in lines[0].split(',')]\n",
    "        start = 1\n",
    "\n",
    "    raw_rows = []\n",
    "    expected_cols = None\n",
    "    for line in lines[start:]:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        parts = [p.strip() for p in line.split(',')]\n",
    "        if expected_cols is None:\n",
    "            expected_cols = len(parts)\n",
    "        if len(parts) != expected_cols:\n",
    "            # skip malformed\n",
    "            continue\n",
    "        row = [_try_float(x) for x in parts]\n",
    "        raw_rows.append(row)\n",
    "\n",
    "    if len(raw_rows) == 0:\n",
    "        print(\"No data rows found after header.\")\n",
    "        return header, None\n",
    "\n",
    "    data = np.array(raw_rows, dtype=float)\n",
    "    return header, data\n",
    "\n",
    "def one_hot_encode_numpy_only(X, feature_names, categorical_indices):\n",
    "    \"\"\"\n",
    "    One-hot encode selected columns (by index) using NumPy only.\n",
    "    Returns (X_new, feature_names_new, one_hot_mask).\n",
    "    \"\"\"\n",
    "    X_list = []\n",
    "    names_new = []\n",
    "    one_hot_mask = []\n",
    "    cat_set = set(categorical_indices)\n",
    "    n, d = X.shape\n",
    "\n",
    "    for j in range(d):\n",
    "        col = X[:, j]\n",
    "        if j in cat_set:\n",
    "            cats = np.unique(col.astype(int))\n",
    "            for c in cats:\n",
    "                oh = (col.astype(int) == c).astype(np.float32).reshape(n, 1)\n",
    "                X_list.append(oh)\n",
    "                names_new.append(f\"{feature_names[j]}=={c}\")\n",
    "                one_hot_mask.append(True)\n",
    "        else:\n",
    "            X_list.append(col.reshape(n, 1))\n",
    "            names_new.append(feature_names[j])\n",
    "            one_hot_mask.append(False)\n",
    "\n",
    "    X_new = np.hstack(X_list).astype(np.float32)\n",
    "    return X_new, names_new, np.array(one_hot_mask, dtype=bool)\n",
    "\n",
    "def load_credit_default_csv_numpy_only(csv_path, assume_header=True, do_one_hot=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Loads the UCI credit default dataset from CSV with NumPy & builtins only.\n",
    "    Returns:\n",
    "        X: (n_samples, n_features) float32\n",
    "        y: (n_samples,) int32 in {0,1}\n",
    "        feature_names: list of names for X columns\n",
    "        one_hot_mask: boolean mask of shape (n_features,)\n",
    "    \"\"\"\n",
    "    header, values = read_csv_numpy_only(csv_path, assume_header=assume_header)\n",
    "    if values is None:\n",
    "        raise ValueError(\"Failed to load data.\")\n",
    "\n",
    "    # Impute NaN with column means\n",
    "    col_means = np.nanmean(values, axis=0)\n",
    "    inds = np.where(np.isnan(values))\n",
    "    values[inds] = col_means[inds[1]]\n",
    "\n",
    "    if header is not None:\n",
    "        raw_names = [h.strip() for h in header]\n",
    "        safe_names = [_safe_name(h) for h in raw_names]\n",
    "        upper_names = [_upper(h) for h in raw_names]\n",
    "        name_to_idx = {n: i for i, n in enumerate(safe_names)}\n",
    "\n",
    "        # Find target\n",
    "        target_idx = None\n",
    "        for cand in KNOWN_TARGET_NAMES:\n",
    "            if cand in name_to_idx:\n",
    "                target_idx = name_to_idx[cand]\n",
    "                break\n",
    "        if target_idx is None:\n",
    "            target_idx = values.shape[1] - 1  # fallback to last column\n",
    "\n",
    "        # Drop ID if present\n",
    "        drop_indices = set()\n",
    "        for i, safe in enumerate(safe_names):\n",
    "            if safe in KNOWN_DROP_COLS:\n",
    "                drop_indices.add(i)\n",
    "\n",
    "        y = values[:, target_idx].astype(np.int32)\n",
    "\n",
    "        keep_mask = np.ones(values.shape[1], dtype=bool)\n",
    "        keep_mask[target_idx] = False\n",
    "        if len(drop_indices) > 0:\n",
    "            for di in drop_indices:\n",
    "                keep_mask[di] = False\n",
    "\n",
    "        X = values[:, keep_mask]\n",
    "        kept_names = [raw_names[i] for i in range(len(raw_names)) if keep_mask[i]]\n",
    "        kept_safe = [safe_names[i] for i in range(len(safe_names)) if keep_mask[i]]\n",
    "        kept_upper = [upper_names[i] for i in range(len(upper_names)) if keep_mask[i]]\n",
    "\n",
    "        # One-hot for categorical\n",
    "        one_hot_mask = np.zeros(X.shape[1], dtype=bool)\n",
    "        if do_one_hot:\n",
    "            cat_indices = [i for i, nm in enumerate(kept_upper) if nm in CATEGORICAL_NAMES]\n",
    "            if len(cat_indices) > 0:\n",
    "                X, new_names, one_hot_mask = one_hot_encode_numpy_only(X, kept_names, cat_indices)\n",
    "                feature_names = new_names\n",
    "            else:\n",
    "                feature_names = kept_names\n",
    "        else:\n",
    "            feature_names = kept_names\n",
    "    else:\n",
    "        # No header: assume last column is target, first is ID to drop\n",
    "        y = values[:, -1].astype(np.int32)\n",
    "        X = values[:, 1:-1]\n",
    "        feature_names = [f\"x{i}\" for i in range(X.shape[1])]\n",
    "        one_hot_mask = np.zeros(X.shape[1], dtype=bool)\n",
    "\n",
    "    if verbose:\n",
    "        pos = int(y.sum())\n",
    "        print(f\"Loaded: X.shape={X.shape}, y.shape={y.shape}, positives={pos} ({100.0*y.mean():.2f}%)\")\n",
    "    return X.astype(np.float32), y.astype(np.int32), feature_names, one_hot_mask\n",
    "\n",
    "# -----------------------------\n",
    "# Train/Test Split (stratified)\n",
    "# -----------------------------\n",
    "def stratified_train_test_split(X, y, test_size=0.2, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    classes = np.unique(y)\n",
    "    train_idx = []\n",
    "    test_idx = []\n",
    "    for c in classes:\n",
    "        idx = np.where(y == c)[0]\n",
    "        rng.shuffle(idx)\n",
    "        n_test = int(round(len(idx) * test_size))\n",
    "        test_idx.append(idx[:n_test])\n",
    "        train_idx.append(idx[n_test:])\n",
    "    train_idx = np.concatenate(train_idx)\n",
    "    test_idx = np.concatenate(test_idx)\n",
    "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "# -----------------------------\n",
    "# Standard Scaler\n",
    "# -----------------------------\n",
    "class StandardScaler:\n",
    "    def __init__(self, mask_scale=None):\n",
    "        self.mean_ = None\n",
    "        self.std_ = None\n",
    "        self.mask_scale = mask_scale\n",
    "\n",
    "    def fit(self, X):\n",
    "        if self.mask_scale is None:\n",
    "            self.mask_scale = np.ones(X.shape[1], dtype=bool)\n",
    "        self.mean_ = np.zeros(X.shape[1], dtype=np.float32)\n",
    "        self.std_ = np.ones(X.shape[1], dtype=np.float32)\n",
    "        cols = np.where(self.mask_scale)[0]\n",
    "        if len(cols) > 0:\n",
    "            self.mean_[cols] = X[:, cols].mean(axis=0)\n",
    "            std = X[:, cols].std(axis=0)\n",
    "            std[std == 0] = 1.0\n",
    "            self.std_[cols] = std\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (X - self.mean_) / self.std_\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "def sigmoid(z):\n",
    "    z = np.clip(z, -500.0, 500.0)\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return (y_true == y_pred).mean()\n",
    "\n",
    "def precision_recall_f1(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    precision = tp / (tp + fp + 1e-12)\n",
    "    recall = tp / (tp + fn + 1e-12)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-12)\n",
    "    return precision, recall, f1\n",
    "\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    return np.array([[tn, fp], [fn, tp]], dtype=np.int32)\n",
    "\n",
    "# -----------------------------\n",
    "# Models\n",
    "# -----------------------------\n",
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.001, epochs=5, shuffle=True, seed=42):\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.w = None  # includes bias at last index\n",
    "\n",
    "    def _add_bias(self, X):\n",
    "        n = X.shape[0]\n",
    "        return np.hstack([X, np.ones((n, 1), dtype=X.dtype)])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_bin = np.where(y == 1, 1, -1).astype(np.int32)\n",
    "        Xb = self._add_bias(X)\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        self.w = np.zeros(Xb.shape[1], dtype=np.float32)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            idx = np.arange(Xb.shape[0])\n",
    "            if self.shuffle:\n",
    "                rng.shuffle(idx)\n",
    "            for i in idx:\n",
    "                xi = Xb[i]\n",
    "                yi = y_bin[i]\n",
    "                if yi * (xi @ self.w) <= 0:\n",
    "                    self.w += self.lr * yi * xi\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        return self._add_bias(X) @ self.w\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.decision_function(X) >= 0).astype(np.int32)\n",
    "\n",
    "class LogisticRegressionGD:\n",
    "    def __init__(self, learning_rate=0.1, epochs=200, l2=1e-4, batch_size=256, seed=42):\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.l2 = l2\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.w = None  # includes bias\n",
    "\n",
    "    def _add_bias(self, X):\n",
    "        n = X.shape[0]\n",
    "        return np.hstack([X, np.ones((n, 1), dtype=X.dtype)])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X).astype(np.float32)\n",
    "        y = y.astype(np.float32)\n",
    "        n, d = Xb.shape\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        self.w = rng.normal(0.0, 0.01, size=d).astype(np.float32)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            if self.batch_size is None:\n",
    "                batches = [(Xb, y)]\n",
    "            else:\n",
    "                idx = np.arange(n)\n",
    "                rng.shuffle(idx)\n",
    "                batches = []\n",
    "                for start in range(0, n, self.batch_size):\n",
    "                    end = min(start + self.batch_size, n)\n",
    "                    bi = idx[start:end]\n",
    "                    batches.append((Xb[bi], y[bi]))\n",
    "\n",
    "            for Xb_batch, y_batch in batches:\n",
    "                logits = Xb_batch @ self.w\n",
    "                p = sigmoid(logits)\n",
    "                grad = (Xb_batch.T @ (p - y_batch)) / Xb_batch.shape[0]\n",
    "                if self.l2 > 0:\n",
    "                    grad[:-1] += self.l2 * self.w[:-1]  # don't regularize bias\n",
    "                self.w -= self.lr * grad\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        Xb = self._add_bias(X)\n",
    "        return sigmoid(Xb @ self.w)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) >= threshold).astype(np.int32)\n",
    "\n",
    "class SimpleNN:\n",
    "    def __init__(self, hidden_units=32, activation='relu', learning_rate=0.01,\n",
    "                 epochs=50, l2=1e-4, batch_size=256, seed=42):\n",
    "        self.h = hidden_units\n",
    "        self.activation = activation\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.l2 = l2\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.W1 = None; self.b1 = None\n",
    "        self.W2 = None; self.b2 = None\n",
    "\n",
    "    def _init_params(self, in_dim):\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        if self.activation == 'relu':\n",
    "            self.W1 = (rng.standard_normal((in_dim, self.h)) * np.sqrt(2.0 / in_dim)).astype(np.float32)\n",
    "        else:\n",
    "            self.W1 = (rng.standard_normal((in_dim, self.h)) * np.sqrt(1.0 / in_dim)).astype(np.float32)\n",
    "        self.b1 = np.zeros((1, self.h), dtype=np.float32)\n",
    "        self.W2 = (rng.standard_normal((self.h, 1)) * np.sqrt(1.0 / self.h)).astype(np.float32)\n",
    "        self.b2 = np.zeros((1, 1), dtype=np.float32)\n",
    "\n",
    "    def _act(self, Z):\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0.0, Z)\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(Z)\n",
    "        else:\n",
    "            raise ValueError(\"activation must be 'relu' or 'tanh'\")\n",
    "\n",
    "    def _act_deriv(self, Z):\n",
    "        if self.activation == 'relu':\n",
    "            return (Z > 0.0).astype(np.float32)\n",
    "        elif self.activation == 'tanh':\n",
    "            A = np.tanh(Z)\n",
    "            return 1.0 - A*A\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = X.astype(np.float32)\n",
    "        y = y.reshape(-1, 1).astype(np.float32)\n",
    "        n, d = X.shape\n",
    "        self._init_params(d)\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            if self.batch_size is None:\n",
    "                batches = [(X, y)]\n",
    "            else:\n",
    "                idx = np.arange(n)\n",
    "                rng.shuffle(idx)\n",
    "                batches = []\n",
    "                for start in range(0, n, self.batch_size):\n",
    "                    end = min(start + self.batch_size, n)\n",
    "                    bi = idx[start:end]\n",
    "                    batches.append((X[bi], y[bi]))\n",
    "\n",
    "            for Xb, yb in batches:\n",
    "                # Forward\n",
    "                Z1 = Xb @ self.W1 + self.b1\n",
    "                A1 = self._act(Z1)\n",
    "                Z2 = A1 @ self.W2 + self.b2\n",
    "                A2 = sigmoid(Z2)\n",
    "\n",
    "                # Backprop (BCE)\n",
    "                m = Xb.shape[0]\n",
    "                dZ2 = (A2 - yb) / m\n",
    "                dW2 = A1.T @ dZ2\n",
    "                db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
    "\n",
    "                dA1 = dZ2 @ self.W2.T\n",
    "                dZ1 = dA1 * self._act_deriv(Z1)\n",
    "                dW1 = Xb.T @ dZ1\n",
    "                db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "                if self.l2 > 0:\n",
    "                    dW2 += self.l2 * self.W2\n",
    "                    dW1 += self.l2 * self.W1\n",
    "\n",
    "                # Update\n",
    "                self.W2 -= self.lr * dW2\n",
    "                self.b2 -= self.lr * db2\n",
    "                self.W1 -= self.lr * dW1\n",
    "                self.b1 -= self.lr * db1\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = X.astype(np.float32)\n",
    "        Z1 = X @ self.W1 + self.b1\n",
    "        A1 = self._act(Z1)\n",
    "        Z2 = A1 @ self.W2 + self.b2\n",
    "        return sigmoid(Z2).ravel()\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) >= threshold).astype(np.int32)\n",
    "\n",
    "# -----------------------------\n",
    "# Run experiment\n",
    "# -----------------------------\n",
    "def run_experiment():\n",
    "    X, y, feature_names, one_hot_mask = load_credit_default_csv_numpy_only(\n",
    "        CSV_PATH, assume_header=True, do_one_hot=True, verbose=True\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = stratified_train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, seed=SEED\n",
    "    )\n",
    "\n",
    "    # Scale numeric (non one-hot) features\n",
    "    if one_hot_mask is not None and one_hot_mask.size == X.shape[1]:\n",
    "        scale_mask = ~one_hot_mask\n",
    "    else:\n",
    "        scale_mask = np.ones(X.shape[1], dtype=bool)\n",
    "\n",
    "    scaler = StandardScaler(mask_scale=scale_mask)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # -------- Perceptron --------\n",
    "    print(\"\\n=== Perceptron ===\")\n",
    "    perceptron = Perceptron(learning_rate=0.001, epochs=5, shuffle=True, seed=SEED)\n",
    "    perceptron.fit(X_train, y_train)\n",
    "    y_pred_p = perceptron.predict(X_test)\n",
    "    acc_p = accuracy_score(y_test, y_pred_p)\n",
    "    pr_p, rc_p, f1_p = precision_recall_f1(y_test, y_pred_p)\n",
    "    cm_p = confusion_matrix(y_test, y_pred_p)\n",
    "    print(f\"Accuracy: {acc_p:.4f} | Precision: {pr_p:.4f} | Recall: {rc_p:.4f} | F1: {f1_p:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", cm_p)\n",
    "\n",
    "    # ---- Logistic Regression ----\n",
    "    print(\"\\n=== Logistic Regression (GD) ===\")\n",
    "    logreg = LogisticRegressionGD(learning_rate=0.1, epochs=200, l2=1e-4, batch_size=256, seed=SEED)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    y_pred_l = logreg.predict(X_test, threshold=0.5)\n",
    "    acc_l = accuracy_score(y_test, y_pred_l)\n",
    "    pr_l, rc_l, f1_l = precision_recall_f1(y_test, y_pred_l)\n",
    "    cm_l = confusion_matrix(y_test, y_pred_l)\n",
    "    print(f\"Accuracy: {acc_l:.4f} | Precision: {pr_l:.4f} | Recall: {rc_l:.4f} | F1: {f1_l:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", cm_l)\n",
    "\n",
    "    # ---------- Simple NN --------\n",
    "    print(\"\\n=== Simple Neural Network (1 hidden layer) ===\")\n",
    "    nn = SimpleNN(hidden_units=32, activation='relu', learning_rate=0.01,\n",
    "                  epochs=50, l2=1e-4, batch_size=256, seed=SEED)\n",
    "    nn.fit(X_train, y_train)\n",
    "    y_pred_n = nn.predict(X_test, threshold=0.5)\n",
    "    acc_n = accuracy_score(y_test, y_pred_n)\n",
    "    pr_n, rc_n, f1_n = precision_recall_f1(y_test, y_pred_n)\n",
    "    cm_n = confusion_matrix(y_test, y_pred_n)\n",
    "    print(f\"Accuracy: {acc_n:.4f} | Precision: {pr_n:.4f} | Recall: {rc_n:.4f} | F1: {f1_n:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", cm_n)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a48ee85-ebf2-4003-ac40-e1b756309bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
